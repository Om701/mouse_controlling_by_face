import mediapipe as mp
import itertools
import matplotlib.pyplot as plt
import cv2
import numpy as np
import time
import pyautogui

# Initialize MediaPipe facial landmark detector
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles
mp_face_mesh = mp.solutions.face_mesh
face_mesh = mp_face_mesh.FaceMesh()


# Initialize PyAutoGUI for controlling the mouse
pyautogui.PAUSE = 0
screen_width, screen_height = pyautogui.size()
mouse_x, mouse_y = screen_width // 2, screen_height // 2
pyautogui.moveTo(mouse_x, mouse_y)


# Define the threshold values for detecting the mouth, eyebrow, and nose movements
mouth_threshold = 0.12
eyebrow_threshold = 0.077
nose_threshold = 0.03


# Define the size of the buffer for storing previous landmark positions
buffer_size = 10

# Define a helper List to control the Cursor's speed_____________________________(?)
d_speed=[0.5,0.75,1,1.5,2,3]

# Define lists to store the previous landmark positions
mouth_buffer = []
left_eyebrow_buffer = []
right_eyebrow_buffer = []
nose_buffer = []
head_buffer = []

# Define a Boolean flag variable for double click
double_click_enabled = False

# Define the duration threshold for smile
duration_threshold = 3.0

# Define a variable to track the duration of smile
smile_duration = 0.0

# Initialize the recording flag
recording = False

# Initialize the start time
#start_time = time.time()

# Define a variable to control the clicking mode
relaxing_mode = False

# Initialize the recording flag
recording = False

# Intialize a variable to control the draging mode
scrolling_mode =False

# Intialize a list to save the recorded actions
recorded_actions = []

# Initialize PyAutoGUI
pyautogui.FAILSAFE = False  # Disable failsafe to prevent accidental termination

# Define a variable to store the smile
mouth_distance = 0

# Left eyebrow

LEFT_EYEBROW_INDEXES = [383, 300, 293, 334, 296, 336, 285, 417]
RIGHT_EYEBROW_INDEXES = [156, 70, 63, 105, 66, 107, 55, 193]
HEAD_INDEXES = [10]
SINGLE_EYEBROW_INDEXES = [296]
NOSE_INDEXES = [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,30,31,32,33]
MOUTH_INDEXES = [61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291]


# Define a helper function to calculate the distance between two points
def distance_between_points(p1, p2):
    return ((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)**0.5

# Define a helper function to check if a point is within the bounds of the screen
def onScreen(x, y):
    return 0 <= x < screen_width and 0 <= y < screen_height


# Define a helper function to start recording actions
def start_recording():
    recorded_actions = []


# Define a helper function to record an action and its position
def record_action(action, position):
    recorded_actions.append((action, position))


# Define a helper function to save recorded actions to a file
def save_recorded_actions(filename):
    with open(filename, "w") as file:
        for action, position in recorded_actions:
            file.write(f"{action},{position[0]},{position[1]}\n")


def replay_recorded_actions(filename):
    with open(filename, "r") as file:
        for line in file:
            action, x, y = line.strip().split(",")
            x = int(x)
            y = int(y)

            # Move the mouse to the recorded position
            pyautogui.moveTo(x, y)

            # Perform the recorded action
            if action == "click_left":
                mouse_click(LEFT)
            elif action == "click_right":
                mouse_click(RIGHT)
            elif action == "double_click":
                mouse_click(LEFT)
                mouse_click(LEFT)

                # Add a small delay between actions
                time.sleep(0.05)

    # Start video capture
cap = cv2.VideoCapture(0)

# check if the video capture object was successfully opened
if not cap.isOpened():
    print("Could not open video capture.")
    exit()

# Create a FaceMesh instance
mp_face_mesh = mp.solutions.face_mesh.FaceMesh(
    min_detection_confidence=0.5,
    min_tracking_confidence=0.5
)

with mp_face_mesh as face_mesh:
    frames_no = 0
    nose_avgs = []
    start_time = time.time()
    while True:
        # Read a frame from the video capture
        ret, image = cap.read()
        image = cv2.flip(image,1)

        if not ret:
            break


        # Convert frame from BGR to RGB
        rgb_frame = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # Process the frame with the FaceMesh model
        processed_frame = face_mesh.process(rgb_frame)

        # Extract the facial landmarks from the MediaPipe results
        if processed_frame.multi_face_landmarks:
            face_landmarks = processed_frame.multi_face_landmarks[0]

            # Get the mouth, eyebrow, and nose landmark positions
            head_positions = [[face_landmarks.landmark[i].x, face_landmarks.landmark[i].y] for i in HEAD_INDEXES]
            mouth_positions = [[face_landmarks.landmark[i].x, face_landmarks.landmark[i].y] for i in MOUTH_INDEXES]
            eyebrow_singel_point_position = [[face_landmarks.landmark[i].x, face_landmarks.landmark[i].y] for i in SINGLE_EYEBROW_INDEXES]
            left_eyebrow_positions = [[face_landmarks.landmark[i].x, face_landmarks.landmark[i].y] for i in LEFT_EYEBROW_INDEXES]
            right_eyebrow_positions = [[face_landmarks.landmark[i].x, face_landmarks.landmark[i].y] for i in RIGHT_EYEBROW_INDEXES]
            nose_position = [[face_landmarks.landmark[i].x, face_landmarks.landmark[i].y] for i in NOSE_INDEXES]

            # Distance
            mouth_buffer_distance = distance_between_points(mouth_positions[0], mouth_positions[-1])
            left_eyebrow_buffer_distance = distance_between_points(left_eyebrow_positions[0], left_eyebrow_positions[-1])
            right_eyebrow_buffer_distance = distance_between_points(right_eyebrow_positions[0], right_eyebrow_positions[-1])
            nose_buffer_distance = distance_between_points(nose_position[0], nose_position[-1])
            single_eyebrow_head_distance = distance_between_points(head_positions[0], eyebrow_singel_point_position[0])


            # Add the current positions to the landmark position buffers
            mouth_buffer.append(mouth_buffer_distance)
            left_eyebrow_buffer.append(left_eyebrow_buffer_distance)
            right_eyebrow_buffer.append(right_eyebrow_buffer_distance)
            nose_buffer.append(nose_buffer_distance)
            head_buffer.append(single_eyebrow_head_distance)

            # If the buffers are full, remove the oldest positions
            if len(mouth_buffer) > buffer_size:
                mouth_buffer.pop(0)
            if len(left_eyebrow_buffer) > buffer_size:
                left_eyebrow_buffer.pop(0)
            if len(right_eyebrow_buffer) > buffer_size:
                right_eyebrow_buffer.pop(0)
            if len(nose_buffer) > buffer_size:
                nose_buffer.pop(0)


            # Define lists to store the previous landmark positions
            mouth_buffer_np =  np.array(mouth_buffer)
            left_eyebrow_buffer_np =  np.array(left_eyebrow_buffer)
            right_eyebrow_buffer_np =  np.array(right_eyebrow_buffer)
            nose_buffer_np =  np.array(nose_buffer)
            head_buffer_np = np.array(head_buffer)

            # Initialize avg nose position
            nose_avgs = [[mouse_x, mouse_y]]

            # Calculate the average positions of the mouth, eyebrows, and nose
            avg_mouth_position = np.mean(mouth_buffer_np)

            avg_left_eyebrow_position = np.mean(left_eyebrow_buffer_np)

            avg_right_eyebrow_position = np.mean(right_eyebrow_buffer_np)

            avg_nose_position = np.mean(nose_buffer_np)


            # Move the cursor to the average nose position
            nose_x = int((nose_position[-1][0]) * screen_width *3)
            nose_y = int((nose_position[-1][-1]) * screen_height*3)
            if onScreen(nose_x, nose_y):
                pyautogui.moveTo(nose_x, nose_y, duration=0.05)
                if recording:
                    record_action("move", (nose_x, nose_y))
                if not scrolling_mode:
                    if nose_position[-1][1] > nose_avgs[0][1]:
                        pyautogui.scroll(1)  # Scroll up
                    elif nose_position[-1][1] < nose_avgs[0][1]:
                        pyautogui.scroll(-1) # Scroll down
                    else:
                        scrolling_mode = not scrolling_mode

                # Check if the clicking mode is on
                if not relaxing_mode:
                    # Check if the left or Right eyebrow is raised
                    print("####################: ", head_buffer[-1])
                    if head_buffer[-1] < eyebrow_threshold:
                        mouse_click(RIGHT)
                        if recording:
                            record_action("RIGHT click", "left eyebrow")


                    # Check if the user is smiling
                    if double_click_enabled:
                        if avg_mouth_position > mouth_threshold:
                            pyautogui.doubleClick()
                            # If recording flag is True, record the action and its position
                            if recording:
                                record_action("click", "mouth")
                    else:
                        if avg_mouth_position > mouth_threshold:
                            current_time = time.time()
                            elapsed_time = current_time - start_time
                            smile_duration += elapsed_time

                            pyautogui.click()
                            # If recording flag is True, record the action and its position
                            if recording:
                                record_action("click", "mouth")

                            if smile_duration > duration_threshold:
                                pyautogui.dragto(nose_x, nose_y)
                                # If recording flag is True, record the action and its position
                                if recording:
                                    record_action("click", "mouth")

                                if mouth_distance < mouth_threshold* screen_width:
                                    pyautogui.mouseUp(button=LEFT)
                                # If recording flag is True, record the action and its position
                                if recording:
                                    record_action("click", "mouth")
                            else:
                                # Reset smile duration if user stops smiling
                                smile_duration = 0.0
                alert = pyautogui.alert('Do you want to cancel The relaxing mode?')
                if alert == 'OK':
                    relaxing_mode = not relaxing_mode



        # Display the image with the facial landmarks
        annotated_image = image.copy()
        # Display the resulting frame
        cv2.imshow('Frame', annotated_image)

        # Check for key presses
        key = cv2.waitKey(1)
        if key == ord('q') or key == 27:
            break


        elif key == ord('r'):
            # Start or stop recording based on the current recording flag
            if recording:
                recording = False
            else:
                record_name = input("Enter the record name")
                save_recorded_actions(record_name+".txt")
                recorded_actions.clear()
                recording = True
                start_recording()


        elif key == ord('s'):
            # Save the recorded actions to a file
            record_name = input("Enter the record name")
            save_recorded_actions(record_name+".txt")
            recorded_actions.clear()
            recording = False


        elif key == ord('d'):
            # Toggle the double click flag
            double_click_enabled = not double_click_enabled

        elif key == ord('w'):
            # Toggle the double click flag
            scrolling_mode = not scrolling_mode

        elif key== ord('p'):
            # Toggle the relaxing mode flag
            relaxing_mode = not relaxing_mode

        elif key == ord('a'):
            # Prompt the user for the path of the recorded actions file
            filename = input("Enter the path of the recorded actions file")

            # Call the replay_recorded_actions function with the provided filename
            replay_recorded_actions(filename)

        frames_no = frames_no + 1

# Release resources
cap.release()
cv2.destroyAllWindows()
